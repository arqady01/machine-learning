寻找一条直线，最大程度拟合样本特征（房屋特征）和样本输出标记（价格）之间的关系

和knn那一章有一个区别，肿瘤的确定与否和两个样本特征有关：肿瘤大小和发现时间，而样本输出标记由是否癌症（单一维度）表示。而本章节横轴是样本特征，纵轴就是样本输出标记了

回归问题中要预测是一个具体的数值，需要占用一个坐标轴；如果要看两个样本特征的回归问题，就要用三维坐标系

样本特征只有一个，称为简单线性回归

# 简单线性回归

通过分析问题，确定问题的损失函数或者效用函数，通过最优化损失函数或者效用函数，获得机器学习的模型

![简单线性回归](https://github.com/arqady01/machine-learning/blob/main/img/linear_regression.png)

假设找到了最佳拟合的直线方程： $y = ax + b$ ，则对于每一个样本点 $x^{(i)}$ ，根据直线方程，预测值为 $\hat{y}^{(i)}=a x^{(i)}+b$ ，真实值为 $y^{i}$ ，二者差距为 $|y^{(i)} - \hat{y}^{(i)}|$ ，但是绝对值表达式不是处处可导的，所以用 ${(y^{(i)} - \hat{y}^{(i)})}^2$ 更贴切，考虑到所有样本，

目标是让 $\sum_{i=1}^{m}(y^{(i)}-\hat{y}^{(i)})^2$ 尽可能小，即找到a和b（x和y是已知的），让 $\sum_{i=1}^{m}(y^{(i)}-ax^{(i)}-b)^2$ 尽可能小，这是典型的最小化二乘法问题

## 最小化二乘法

目标：找到a和b，使得

$$\sum_{i=1}^{m}(y^{(i)}-ax^{(i)}-b)^2$$

最小，不妨让上式表示为 $J(a,b)$ ，分别对a和b求偏导并让其等于0，得到：

$$
\
\begin{cases}
\frac{\partial J(a,b)}{\partial a} = \sum_{i=1}^{m} 2(y^{(i)} - ax^{(i)} - b)(-x^{(i)}) = 0 \\
\frac{\partial J(a,b)}{\partial b} = \sum_{i=1}^{m} 2(y^{(i)} - ax^{(i)} - b)(-1) = 0
\end{cases}
\
$$

进一步求出b再带入第一行式子，最终得到：

$$
\
\begin{cases}
a=\frac{ {\textstyle \sum_{i=1}^{m}(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}}{ {\textstyle \sum_{i=1}^{m}(x^{(i)}-\bar{x})^{2}} }  \\
b = \bar{y}-a\bar{x}
\end{cases}
\
$$

## 简单线性回归实现

```python
import numpy as np
import matplotlib.pyplot as plt
x = np.array([1., 2., 3., 4., 5.])
y = np.array([1., 3., 2., 3., 5.])
plt.scatter(x, y) # 绘制
plt.axis([0, 6, 0, 6]) # 规定横纵坐标的范围
plt.show()
```

![点坐标](https://github.com/arqady01/machine-learning/blob/main/img/Figure_1.png)

根据公式求a和b的值

```python
x_mean = np.mean(x) # x的均值
y_mean = np.mean(y) # y的均值
fenzi = 0.0 # 初始化分子
fenmu = 0.0 # 初始化分母
for x_i, y_i in zip(x, y): # 每次分别从zip(x,y)向量中取出值
    fenzi += (x_i - x_mean) * (y_i - y_mean) # 公式中分子的求法
    fenmu += (x_i - x_mean) ** 2 # 公式中分母的求法

a = fenzi / fenmu # 输出：0.8
b = y_mean - a * x_mean # 输出：0.39999999999999947

# 直接写出线的表达式
y_hat = a * x + b # 预测值
# 画出线
plt.plot(x, y_hat, color='r')
plt.show()
```

![figure2](https://github.com/arqady01/machine-learning/blob/main/img/Figure_2.png)
