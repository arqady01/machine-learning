寻找一条直线，最大程度拟合样本特征（房屋特征）和样本输出标记（价格）之间的关系

和knn那一章有一个区别，肿瘤的确定与否和两个样本特征有关：肿瘤大小和发现时间，而样本输出标记由是否癌症（单一维度）表示。而本章节横轴是样本特征，纵轴就是样本输出标记了

回归问题中要预测是一个具体的数值，需要占用一个坐标轴；如果要看两个样本特征的回归问题，就要用三维坐标系

样本特征只有一个，称为简单线性回归

# 简单线性回归

通过分析问题，确定问题的损失函数或者效用函数，通过最优化损失函数或者效用函数，获得机器学习的模型

![简单线性回归](https://github.com/arqady01/machine-learning/blob/main/img/linear_regression.png)

假设找到了最佳拟合的直线方程： $y = ax + b$ ，则对于每一个样本点 $x^{(i)}$ ，根据直线方程，预测值为 $\hat{y}^{(i)}=a x^{(i)}+b$ ，真实值为 $y^{i}$ ，二者差距为 $|y^{(i)} - \hat{y}^{(i)}|$ ，但是绝对值表达式不是处处可导的，所以用 ${(y^{(i)} - \hat{y}^{(i)})}^2$ 更贴切，考虑到所有样本，

目标是让 $\sum_{i=1}^{m}(y^{(i)}-\hat{y}^{(i)})^2$ 尽可能小，即找到a和b（x和y是已知的），让 $\sum_{i=1}^{m}(y^{(i)}-ax^{(i)}-b)^2$ 尽可能小，这是典型的最小化二乘法问题

## 最小化二乘法

目标：找到a和b，使得

$$\sum_{i=1}^{m}(y^{(i)}-ax^{(i)}-b)^2$$

最小，不妨让上式表示为 $J(a,b)$ ，分别对a和b求偏导并让其等于0，得到：

$$
\
\begin{cases}
\frac{\partial J(a,b)}{\partial a} = \sum_{i=1}^{m} 2(y^{(i)} - ax^{(i)} - b)(-x^{(i)}) = 0 \\
\frac{\partial J(a,b)}{\partial b} = \sum_{i=1}^{m} 2(y^{(i)} - ax^{(i)} - b)(-1) = 0
\end{cases}
\
$$

进一步求出b再带入第一行式子，最终得到：

$$
\begin{cases}
a=\frac{ {\textstyle \sum_{i=1}^{m}(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}}{ {\textstyle \sum_{i=1}^{m}(x^{(i)}-\bar{x})^{2}} }  \\
b = \bar{y}-a\bar{x}
\end{cases}
\qquad (1-1)
$$

## 简单线性回归实现

```python
import numpy as np
import matplotlib.pyplot as plt
x = np.array([1., 2., 3., 4., 5.])
y = np.array([1., 3., 2., 3., 5.])
plt.scatter(x, y) # 绘制
plt.axis([0, 6, 0, 6]) # 规定横纵坐标的范围
plt.show()
```

![点坐标](https://github.com/arqady01/machine-learning/blob/main/img/Figure_1.png)

根据公式1-1求a和b的值

```python
x_mean = np.mean(x) # x的均值
y_mean = np.mean(y) # y的均值
fenzi = 0.0 # 初始化分子
fenmu = 0.0 # 初始化分母
for x_i, y_i in zip(x, y): # 每次分别从zip(x,y)向量中取出值
    fenzi += (x_i - x_mean) * (y_i - y_mean) # 公式中分子的求法
    fenmu += (x_i - x_mean) ** 2 # 公式中分母的求法

a = fenzi / fenmu # 输出：0.8
b = y_mean - a * x_mean # 输出：0.39999999999999947

# 直接写出线的表达式
y_hat = a * x + b # 预测值
# 画出线
plt.plot(x, y_hat, color='r')
plt.show()
```

![figure2](https://github.com/arqady01/machine-learning/blob/main/img/Figure_2.png)

至此全部完毕，不妨封装一下fit函数：

```python
def fit(x, y):
    x_mean = np.mean(x) # x的均值
    y_mean = np.mean(y) # y的均值
    fenzi = 0.0 # 初始化分子
    fenmu = 0.0 # 初始化分母
    for x_i, y_i in zip(x, y): # 每次分别从zip(x,y)向量中取出值
        fenzi += (x_i - x_mean) * (y_i - y_mean) # 公式中分子的求法
        fenmu += (x_i - x_mean) ** 2 # 公式中分母的求法
    
    a = fenzi / fenmu
    b = y_mean - a * x_mean
```
predict预测函数：
```python
# 接收一个数
def _predict(x_single):
    return a * x_single + b
# 接收一个一维向量，返回一个向量
def predict(x_predict):
    return np.array([_predict(i) for i in x_predict])
```

## 向量化运算

观察

$$
\
\begin{cases}
a=\frac{ {\textstyle \sum_{i=1}^{m}(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}}{ {\textstyle \sum_{i=1}^{m}(x^{(i)}-\bar{x})^{2}} }  \\
b = \bar{y}-a\bar{x}
\end{cases}
\
$$

因为是一维向量，所以相乘再相加，等价于两个向量之间点乘，所以用点乘就可以替代for循环相乘，提升性能

```python
def fit(x, y):
    x_mean = np.mean(x) # x的均值
    y_mean = np.mean(y) # y的均值
    fenzi = (x - x_mean).dot(y - y_mean) # 初始化分子
    fenmu = (x - x_mean).dot(x - x_mean) # 初始化分母
    
    a = fenzi / fenmu
    b = y_mean - a * x_mean
```

## 误差

误差ε是独立并且具有相同的分布，并且服从均值为0方差为θ^2的高斯分布

- 独立：张三和李四一起来贷款，他俩没关系
- 同分布：他俩都来得是我们假定的这家银行
- 正态（高斯）分布：银行可能会多给，也可能会少给，但是绝大多数情况下这个浮动不会太大，极小情况下浮动会比较大，符合正常情况

$$
f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
$$

其中：

- 𝜇 是均值（mean）
- $𝜎^2$ 是方差（variance）
- 𝜎 是标准差（standard deviation）

而均值为0，即 $𝜇 = 0$ ，所以误差服从高斯分布可以表示为

$$
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{\epsilon^2}{2\sigma^2}}
\qquad (2-1)
$$

而预测值与误差的关系为：

$$
y^{(i)} = θ^{(T)}x^{(i)} + ε^{(i)}
\qquad (2-2)
$$
