å¯»æ‰¾ä¸€æ¡ç›´çº¿ï¼Œæœ€å¤§ç¨‹åº¦æ‹Ÿåˆæ ·æœ¬ç‰¹å¾ï¼ˆæˆ¿å±‹ç‰¹å¾ï¼‰å’Œæ ·æœ¬è¾“å‡ºæ ‡è®°ï¼ˆä»·æ ¼ï¼‰ä¹‹é—´çš„å…³ç³»

å’Œknné‚£ä¸€ç« æœ‰ä¸€ä¸ªåŒºåˆ«ï¼Œè‚¿ç˜¤çš„ç¡®å®šä¸å¦å’Œä¸¤ä¸ªæ ·æœ¬ç‰¹å¾æœ‰å…³ï¼šè‚¿ç˜¤å¤§å°å’Œå‘ç°æ—¶é—´ï¼Œè€Œæ ·æœ¬è¾“å‡ºæ ‡è®°ç”±æ˜¯å¦ç™Œç—‡ï¼ˆå•ä¸€ç»´åº¦ï¼‰è¡¨ç¤ºã€‚è€Œæœ¬ç« èŠ‚æ¨ªè½´æ˜¯æ ·æœ¬ç‰¹å¾ï¼Œçºµè½´å°±æ˜¯æ ·æœ¬è¾“å‡ºæ ‡è®°äº†

å›å½’é—®é¢˜ä¸­è¦é¢„æµ‹æ˜¯ä¸€ä¸ªå…·ä½“çš„æ•°å€¼ï¼Œéœ€è¦å ç”¨ä¸€ä¸ªåæ ‡è½´ï¼›å¦‚æœè¦çœ‹ä¸¤ä¸ªæ ·æœ¬ç‰¹å¾çš„å›å½’é—®é¢˜ï¼Œå°±è¦ç”¨ä¸‰ç»´åæ ‡ç³»

æ ·æœ¬ç‰¹å¾åªæœ‰ä¸€ä¸ªï¼Œç§°ä¸ºç®€å•çº¿æ€§å›å½’

# ç®€å•çº¿æ€§å›å½’

é€šè¿‡åˆ†æé—®é¢˜ï¼Œç¡®å®šé—®é¢˜çš„æŸå¤±å‡½æ•°æˆ–è€…æ•ˆç”¨å‡½æ•°ï¼Œé€šè¿‡æœ€ä¼˜åŒ–æŸå¤±å‡½æ•°æˆ–è€…æ•ˆç”¨å‡½æ•°ï¼Œè·å¾—æœºå™¨å­¦ä¹ çš„æ¨¡å‹

![ç®€å•çº¿æ€§å›å½’](https://github.com/arqady01/machine-learning/blob/main/img/linear_regression.png)

å‡è®¾æ‰¾åˆ°äº†æœ€ä½³æ‹Ÿåˆçš„ç›´çº¿æ–¹ç¨‹ï¼š $y = ax + b$ ï¼Œåˆ™å¯¹äºæ¯ä¸€ä¸ªæ ·æœ¬ç‚¹ $x^{(i)}$ ï¼Œæ ¹æ®ç›´çº¿æ–¹ç¨‹ï¼Œé¢„æµ‹å€¼ä¸º $\hat{y}^{(i)}=a x^{(i)}+b$ ï¼ŒçœŸå®å€¼ä¸º $y^{i}$ ï¼ŒäºŒè€…å·®è·ä¸º $|y^{(i)} - \hat{y}^{(i)}|$ ï¼Œä½†æ˜¯ç»å¯¹å€¼è¡¨è¾¾å¼ä¸æ˜¯å¤„å¤„å¯å¯¼çš„ï¼Œæ‰€ä»¥ç”¨ ${(y^{(i)} - \hat{y}^{(i)})}^2$ æ›´è´´åˆ‡ï¼Œè€ƒè™‘åˆ°æ‰€æœ‰æ ·æœ¬ï¼Œ

ç›®æ ‡æ˜¯è®© $\sum_{i=1}^{m}(y^{(i)}-\hat{y}^{(i)})^2$ å°½å¯èƒ½å°ï¼Œå³æ‰¾åˆ°aå’Œbï¼ˆxå’Œyæ˜¯å·²çŸ¥çš„ï¼‰ï¼Œè®© $\sum_{i=1}^{m}(y^{(i)}-ax^{(i)}-b)^2$ å°½å¯èƒ½å°ï¼Œè¿™æ˜¯å…¸å‹çš„æœ€å°åŒ–äºŒä¹˜æ³•é—®é¢˜

## æœ€å°åŒ–äºŒä¹˜æ³•

ç›®æ ‡ï¼šæ‰¾åˆ°aå’Œbï¼Œä½¿å¾—

$$\sum_{i=1}^{m}(y^{(i)}-ax^{(i)}-b)^2$$

æœ€å°ï¼Œä¸å¦¨è®©ä¸Šå¼è¡¨ç¤ºä¸º $J(a,b)$ ï¼Œåˆ†åˆ«å¯¹aå’Œbæ±‚åå¯¼å¹¶è®©å…¶ç­‰äº0ï¼Œå¾—åˆ°ï¼š

$$
\
\begin{cases}
\frac{\partial J(a,b)}{\partial a} = \sum_{i=1}^{m} 2(y^{(i)} - ax^{(i)} - b)(-x^{(i)}) = 0 \\
\frac{\partial J(a,b)}{\partial b} = \sum_{i=1}^{m} 2(y^{(i)} - ax^{(i)} - b)(-1) = 0
\end{cases}
\
$$

è¿›ä¸€æ­¥æ±‚å‡ºbå†å¸¦å…¥ç¬¬ä¸€è¡Œå¼å­ï¼Œæœ€ç»ˆå¾—åˆ°ï¼š

$$
\begin{cases}
a=\frac{ {\textstyle \sum_{i=1}^{m}(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}}{ {\textstyle \sum_{i=1}^{m}(x^{(i)}-\bar{x})^{2}} }  \\
b = \bar{y}-a\bar{x}
\end{cases}
\qquad (1-1)
$$

## ç®€å•çº¿æ€§å›å½’å®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
x = np.array([1., 2., 3., 4., 5.])
y = np.array([1., 3., 2., 3., 5.])
plt.scatter(x, y) # ç»˜åˆ¶
plt.axis([0, 6, 0, 6]) # è§„å®šæ¨ªçºµåæ ‡çš„èŒƒå›´
plt.show()
```

![ç‚¹åæ ‡](https://github.com/arqady01/machine-learning/blob/main/img/Figure_1.png)

æ ¹æ®å…¬å¼1-1æ±‚aå’Œbçš„å€¼

```python
x_mean = np.mean(x) # xçš„å‡å€¼
y_mean = np.mean(y) # yçš„å‡å€¼
fenzi = 0.0 # åˆå§‹åŒ–åˆ†å­
fenmu = 0.0 # åˆå§‹åŒ–åˆ†æ¯
for x_i, y_i in zip(x, y): # æ¯æ¬¡åˆ†åˆ«ä»zip(x,y)å‘é‡ä¸­å–å‡ºå€¼
    fenzi += (x_i - x_mean) * (y_i - y_mean) # å…¬å¼ä¸­åˆ†å­çš„æ±‚æ³•
    fenmu += (x_i - x_mean) ** 2 # å…¬å¼ä¸­åˆ†æ¯çš„æ±‚æ³•

a = fenzi / fenmu # è¾“å‡ºï¼š0.8
b = y_mean - a * x_mean # è¾“å‡ºï¼š0.39999999999999947

# ç›´æ¥å†™å‡ºçº¿çš„è¡¨è¾¾å¼
y_hat = a * x + b # é¢„æµ‹å€¼
# ç”»å‡ºçº¿
plt.plot(x, y_hat, color='r')
plt.show()
```

![figure2](https://github.com/arqady01/machine-learning/blob/main/img/Figure_2.png)

è‡³æ­¤å…¨éƒ¨å®Œæ¯•ï¼Œä¸å¦¨å°è£…ä¸€ä¸‹fitå‡½æ•°ï¼š

```python
def fit(x, y):
    x_mean = np.mean(x) # xçš„å‡å€¼
    y_mean = np.mean(y) # yçš„å‡å€¼
    fenzi = 0.0 # åˆå§‹åŒ–åˆ†å­
    fenmu = 0.0 # åˆå§‹åŒ–åˆ†æ¯
    for x_i, y_i in zip(x, y): # æ¯æ¬¡åˆ†åˆ«ä»zip(x,y)å‘é‡ä¸­å–å‡ºå€¼
        fenzi += (x_i - x_mean) * (y_i - y_mean) # å…¬å¼ä¸­åˆ†å­çš„æ±‚æ³•
        fenmu += (x_i - x_mean) ** 2 # å…¬å¼ä¸­åˆ†æ¯çš„æ±‚æ³•
    
    a = fenzi / fenmu
    b = y_mean - a * x_mean
```
predicté¢„æµ‹å‡½æ•°ï¼š
```python
# æ¥æ”¶ä¸€ä¸ªæ•°
def _predict(x_single):
    return a * x_single + b
# æ¥æ”¶ä¸€ä¸ªä¸€ç»´å‘é‡ï¼Œè¿”å›ä¸€ä¸ªå‘é‡
def predict(x_predict):
    return np.array([_predict(i) for i in x_predict])
```

## å‘é‡åŒ–è¿ç®—

è§‚å¯Ÿ

$$
\
\begin{cases}
a=\frac{ {\textstyle \sum_{i=1}^{m}(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}}{ {\textstyle \sum_{i=1}^{m}(x^{(i)}-\bar{x})^{2}} }  \\
b = \bar{y}-a\bar{x}
\end{cases}
\
$$

å› ä¸ºæ˜¯ä¸€ç»´å‘é‡ï¼Œæ‰€ä»¥ç›¸ä¹˜å†ç›¸åŠ ï¼Œç­‰ä»·äºä¸¤ä¸ªå‘é‡ä¹‹é—´ç‚¹ä¹˜ï¼Œæ‰€ä»¥ç”¨ç‚¹ä¹˜å°±å¯ä»¥æ›¿ä»£forå¾ªç¯ç›¸ä¹˜ï¼Œæå‡æ€§èƒ½

```python
def fit(x, y):
    x_mean = np.mean(x) # xçš„å‡å€¼
    y_mean = np.mean(y) # yçš„å‡å€¼
    fenzi = (x - x_mean).dot(y - y_mean) # åˆå§‹åŒ–åˆ†å­
    fenmu = (x - x_mean).dot(x - x_mean) # åˆå§‹åŒ–åˆ†æ¯
    
    a = fenzi / fenmu
    b = y_mean - a * x_mean
```

## è¯¯å·®

è¯¯å·®Îµæ˜¯ç‹¬ç«‹å¹¶ä¸”å…·æœ‰ç›¸åŒçš„åˆ†å¸ƒï¼Œå¹¶ä¸”æœä»å‡å€¼ä¸º0æ–¹å·®ä¸ºÎ¸^2çš„é«˜æ–¯åˆ†å¸ƒ

- ç‹¬ç«‹ï¼šå¼ ä¸‰å’Œæå››ä¸€èµ·æ¥è´·æ¬¾ï¼Œä»–ä¿©æ²¡å…³ç³»
- åŒåˆ†å¸ƒï¼šä»–ä¿©éƒ½æ¥å¾—æ˜¯æˆ‘ä»¬å‡å®šçš„è¿™å®¶é“¶è¡Œ
- æ­£æ€ï¼ˆé«˜æ–¯ï¼‰åˆ†å¸ƒï¼šé“¶è¡Œå¯èƒ½ä¼šå¤šç»™ï¼Œä¹Ÿå¯èƒ½ä¼šå°‘ç»™ï¼Œä½†æ˜¯ç»å¤§å¤šæ•°æƒ…å†µä¸‹è¿™ä¸ªæµ®åŠ¨ä¸ä¼šå¤ªå¤§ï¼Œæå°æƒ…å†µä¸‹æµ®åŠ¨ä¼šæ¯”è¾ƒå¤§ï¼Œç¬¦åˆæ­£å¸¸æƒ…å†µ

$$
f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
$$

å…¶ä¸­ï¼š

- ğœ‡ æ˜¯å‡å€¼ï¼ˆmeanï¼‰
- $ğœ^2$ æ˜¯æ–¹å·®ï¼ˆvarianceï¼‰
- ğœ æ˜¯æ ‡å‡†å·®ï¼ˆstandard deviationï¼‰

è€Œå‡å€¼ä¸º0ï¼Œå³ $ğœ‡ = 0$ ï¼Œæ‰€ä»¥è¯¯å·®æœä»é«˜æ–¯åˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸º

$$
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{\epsilon^2}{2\sigma^2}}
\qquad (2-1)
$$

è€Œé¢„æµ‹å€¼ä¸è¯¯å·®çš„å…³ç³»ä¸ºï¼š

$$
y^{(i)} = Î¸^{(T)}x^{(i)} + Îµ^{(i)}
\qquad (2-2)
$$
